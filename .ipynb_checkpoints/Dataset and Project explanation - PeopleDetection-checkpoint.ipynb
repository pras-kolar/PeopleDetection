{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />\n",
    "<img style=\" float:right; display:inline\" src=\"http://opencloud.utsa.edu/wp-content/themes/utsa-oci/images/logo.png\"/>\n",
    "\n",
    "### **University of Texas at San Antonio** \n",
    "<br/>\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 2.5em;\"> **Open Cloud Institute** </span>\n",
    "\n",
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Machine Learning/BigData EE-6973-001-Fall-2016\n",
    "\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Jonathan Lwowski** </span>  \n",
    "\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Prasanna Kolar** </span>\n",
    "  \n",
    "\n",
    "<hr style=\"height:1.5px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:1.5px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 2em;\"> **Machine learning based people detector using UAVs** </span>  \n",
    "<br/>\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.6em;\"> Jonathan Lwowski, Prasanna Kolar </span>  \n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.4em;\"> *Autonomous Controls Lab, University of Texas at San Antonio, San Antonio, Texas, USA* </span>  \n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.4em;\"> {kyx930,vdd121}@my.utsa.edu </span>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Dataset:** </span> <span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> The image data can be found in [http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Mono_Ped__Class__Bench/daimler_mono_ped__class__bench.html] [1]. This dataset contains a collection of pedestrian and non-pedestrian images. The base data set contains a total of 4000 pedestrian and 5000 non-pedestrian samples cut out from video images and scaled to common size of 18x36 pixels. </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Outcome:** </span> <span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Applying deep learning to identify people in a camera's image </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Project Definition:** </span> <span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Using deep neural network models, ResNet and Convnet, classification of camera images of faces of various people in various poses is sudied. The dataset includes images of 20 different people, approximately 32 images per person, varying the person's expression (happy, sad, angry, neutral), the direction in which they are looking (left, right, straight ahead, up), and whether or not they were wearing sunglassess. </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> There is also variation in the background behind the person, the clothing worn by the person, and theposition of the person's face within the image. I total. 624 greyscale images were collected, each whithin a resolutoin of 120by128, with each image pixel described by a greyscale intensity value between 0 (black) and 255 (white).</span>\n",
    "\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> A variety of target function can be learn from this image data. For example, given an image as input we could train a model to output the identity of the person, the direction in which the person is facing, the gender of the person, whether or not they are wearning sunglasses, etc. All of this target functions can be learned to high accuracy from this image data. In this course research we consider the particular task: learning the direction in which the person is facing (to their left, right, straight ahead, or upward)[1]. </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"width:830; background-color:white; height:220px; overflow:scroll; overflow-x: scroll;overflow-y: hidden;\">\n",
    "\n",
    "<img align=\"center\" src=\"http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/dc_ped_class_benchmark.gif\"/>\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]: S. Munder and D. M. Gavrila, \"An Experimental Study on Pedestrian\n",
    "  Classification\", IEEE Trans. on Pattern Analysis and Machine\n",
    "  Intelligence, 2006.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
